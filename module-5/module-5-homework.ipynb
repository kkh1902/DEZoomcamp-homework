{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce90824-0b67-46e2-901b-bf9a17766152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.1.1.tar.gz (455.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.4/455.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:40\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j<0.10.9.10,>=0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.1.1-py2.py3-none-any.whl size=456008705 sha256=40018d987fb26bb18ee8cecfe22944d969e5b7412f07b6bbfe56e72b4fdf6e10\n",
      "  Stored in directory: /home/pc/.cache/pip/wheels/f4/ca/ea/203f40b3e935bbf99bee851c2f4a87d22996ab8212d367ce58\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]━━━\u001b[0m \u001b[32m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.9 pyspark-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58e313-777d-41d4-a1cc-45bdfa791903",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce63c8a4-2f93-47cc-b734-90faf7a8e62b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/22 08:58:54 WARN Utils: Your hostname, DESKTOP-G33DRVE resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/22 08:58:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/22 08:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(f\"The spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d53ca5-97fa-44f9-b2a0-50a632d7ff7c",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e4f2f4-5513-4770-ab6b-7dfac854c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parquet File Size: 22.39 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the dataset\n",
    "file_path = \"yellow_tripdata_2024-10.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# repartition and save as Parquet\n",
    "output_path = \"output/yellow_tripdata_partitioned\"\n",
    "df.repartition(4).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "\n",
    "# Calculate the average size of Parquet files\n",
    "import os\n",
    "\n",
    "file_sizes = [os.path.getsize(os.path.join(output_path, f)) for f in os.listdir(output_path) if f.endswith(\".parquet\")]\n",
    "avg_size_mb = sum(file_sizes) / len(file_sizes) / (1024 * 1024)\n",
    "print(f\"Average Parquet File Size: {avg_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de23ffd-0df5-4f19-b740-8659dc77acef",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01cb6b0b-6612-48ca-8e13-5525ad1b0ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips on October 15: 128893\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Filter trips that started on October 15th\n",
    "df_filtered = df.filter(col(\"tpep_pickup_datetime\").substr(1, 10) == \"2024-10-15\")\n",
    "\n",
    "print(\"Trips on October 15:\", df_filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396d423-f1cc-44f0-99f8-4696bbd211e9",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bb0069-45ec-404e-8440-9a5bb51f368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/spark/spark-3.5.0/python/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|max(timestampdiff(HOUR, tpep_pickup_datetime, tpep_dropoff_datetime))|\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                  162|\n",
      "+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Longest trip\n",
    "\n",
    "df.registerTempTable('trips_data')\n",
    "spark.sql(\"\"\"\n",
    "select MAX(timestampdiff(HOUR, tpep_pickup_datetime, tpep_dropoff_datetime))\n",
    "from trips_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eba78e-3d74-4afe-979e-8bdf49562e12",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc4214b-fc18-4040-b9ca-00414f386fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI runs on port: 4040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark UI Port\n",
    "print(\"Spark UI runs on port: 4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ac7c06-35fc-4003-a671-0a294627094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-22 08:59:22--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 52.84.167.134, 52.84.167.55, 52.84.167.2, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|52.84.167.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2026-01-22 08:59:23 (11.7 MB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffb1a4-9e47-4eec-9615-f9c9656861ad",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23f6dd6-cce1-4d57-a3b3-57256560f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Frequent Pickup Location Zone: Governor's Island/Ellis Island/Liberty Island\n"
     ]
    }
   ],
   "source": [
    "# Least frequent pickup location zone\n",
    "zone_lookup = spark.read.csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "df.createOrReplaceTempView(\"trips\")\n",
    "zone_lookup.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "least_frequent_zone = spark.sql(\"\"\"\n",
    "    SELECT zones.Zone, COUNT(*) as trip_count\n",
    "    FROM trips\n",
    "    JOIN zones ON trips.PULocationID = zones.LocationID\n",
    "    GROUP BY zones.Zone\n",
    "    ORDER BY trip_count ASC\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "\n",
    "print(\"Least Frequent Pickup Location Zone:\", least_frequent_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e09b64-6b97-4408-87e1-442585059ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
